---
title: ffmpeg
pubDatetime: 2024-08-05T01:33:57Z
postSlug: ffmpeg
featured: false
draft: false
tags:
  - ffmpeg
  - 音频
  - 视频
description: "用于格式转换、基本编辑（如剪切和合并）、视频缩放、后期制作效果以及标准合规性"
---

## 目录

## mac安装

```shell
brew install ffmpeg
```

```shell
Running `brew update --auto-update`...
==> Fetching dependencies for ffmpeg: brotli, highway, imath, openexr, webp, jpeg-xl, libvmaf, aom, aribb24, dav1d, frei0r, gmp, libtasn1, nettle, p11-kit, libevent, libnghttp2, unbound, gnutls, libx11, lame, fribidi, libunibreak, libass, libbluray, cjson, libmicrohttpd, mbedtls, librist, libsoxr, libssh, libvidstab, libogg, libvorbis, libvpx, opencore-amr, openjpeg, opus, rav1e, libsamplerate, flac, mpg123, libsndfile, rubberband, sdl2, snappy, speex, srt, svt-av1, leptonica, libb2, libarchive, pango, tesseract, theora, x264, x265, xvid, libsodium, zeromq and zimg
==> Fetching brotli
...
```

```shell
which ffmpeg
/opt/homebrew/bin/ffmpeg
```

## 使用

- 格式转换

<details>
<summary>ffmpeg将图片格式转换成webp</summary>
FFmpeg能够将多种图片格式转换为WebP格式.WebP是一种由Google开发的图像格式,支持有损和无损压缩,适合用于网页图像.

要使用FFmpeg将图片转换为WebP格式,可以使用以下命令：

```bash
ffmpeg -i input_image.png -c:v libwebp -q:v 80 output_image.webp
```

在这个命令中：

- `-i input_image.png` 指定输入文件,可以是任何支持的图像格式（如PNG、JPEG等）.
- `-c:v libwebp` 指定使用WebP编码器.
- `-q:v 80` 设置输出图像的质量,范围是0到100,数值越高质量越好,文件越大.
- `output_image.webp` 是输出文件的名称.

如果你想将一系列图片转换为一个动画WebP文件,可以使用以下命令：

```bash
ffmpeg -r 10 -i image_%03d.png -c:v libwebp -loop 0 -q:v 80 output_animation.webp
```

在这个命令中：

- `-r 10` 设置帧率为每秒10帧.
- `-i image_%03d.png` 指定输入文件名的模式,表示输入文件名为`image_001.png`、`image_002.png`等.
- `-loop 0` 表示动画循环播放.

通过这些命令,FFmpeg可以方便地将图片格式转换为WebP格式,满足不同的需求[1](https://www.tecmint.com/convert-images-to-webp-format-in-linux/).

</details>

## 阅读《深入理解ffmpeg》

### 1.1.1 颜色与图像

<details>
<summary>如何理解CMYK色彩空间</summary>
CMYK色彩空间是一种减法色彩模型,主要用于印刷行业.CMYK代表四种颜色：青色（Cyan）、品红色（Magenta）、黄色（Yellow）和黑色（Key或Black）.在CMYK模型中,颜色是通过在白色背景上叠加不同量的墨水来实现的,每种颜色的墨水会吸收特定的光波长,从而减去白色光的某些部分,形成所需的颜色.

CMYK色彩空间的工作原理是基于减法混合.当将青色、品红色和黄色墨水混合时,理论上可以产生黑色,但由于实际墨水的特性,混合后的颜色通常会呈现出一种暗棕色.因此,为了获得更深的黑色和更高的细节表现,CMYK模型中引入了黑色墨水（K）,这也是CMYK名称中“K”的来源[1](https://www.dusted.com/insights/rgb-vs-cmyk-colour-spaces-explained)[3](https://www.stptexas.com/blog/what-is-cmyk%C2%A0).

CMYK色彩空间的一个重要特点是其色域（即可以表示的颜色范围）相对较小,尤其是与RGB色彩空间相比.RGB是加法色彩模型,主要用于数字显示设备,通过将红色、绿色和蓝色光以不同强度混合来生成颜色.由于RGB模型可以产生更丰富、更饱和的颜色,许多设计师在创建数字内容时会使用RGB,但在准备印刷文件时则需要将其转换为CMYK,以确保打印效果的准确性[2](https://www.printingcenterusa.com/blog/what-is-cmyk-and-why-is-it-used-for-printing/).

理解CMYK色彩空间对于设计师和印刷专业人士至关重要,因为它直接影响到印刷品的颜色表现和质量.在设计过程中,使用CMYK色彩模式可以帮助确保最终印刷效果与设计意图相符,避免因色彩转换而导致的失真或不一致[4](https://intranet.mcad.edu/kb/cmyk-vs-rgb-what-color-space-should-i-work).

</details>

<details>
<summary>如何理解计算机对整数的计算比一个一个字节计算高效</summary>
计算机在处理整数时,通常使用固定大小的字长（如32位或64位）来进行运算,而不是逐字节处理.这种设计的高效性主要源于几个方面.

首先,现代计算机的硬件架构是基于字长进行优化的.CPU内部有专门的算术逻辑单元（ALU）,可以在一个时钟周期内处理一个完整的字（如32位或64位）.如果逐字节处理,计算机需要进行多次操作才能完成同样的任务,这会显著增加计算时间和复杂性.例如,处理一个64位整数需要8个字节的操作,而直接处理64位整数只需一次操作.

其次,使用字长进行运算可以减少内存访问次数.内存访问是计算机中最慢的操作之一.如果每次运算都需要从内存中读取和写入多个字节,性能将受到严重影响.通过使用字长,计算机可以在一次内存访问中获取所需的数据,从而提高效率.

最后,整数运算的硬件实现通常比逐字节运算更为简单和高效.现代处理器设计中,整数运算（如加法、乘法和位操作）都是通过硬件电路直接实现的,而逐字节运算可能需要额外的控制逻辑和更多的指令,这进一步降低了效率.

综上所述,计算机对整数的计算比逐字节计算高效,主要是因为硬件优化、减少内存访问和简化运算逻辑等因素共同作用的结果.这种设计使得计算机能够快速而高效地处理大量数据.

</details>

<details>
<summary>如何理解不同计算机的体系结构是不一样的,主要表现在字节序上</summary>
不同计算机的体系结构确实存在显著差异,其中一个主要表现就是字节序（Byte Order）.字节序指的是在存储多字节数据时,字节的排列顺序.主要有两种字节序：大端序（Big Endian）和小端序（Little Endian）.

在大端序中,数据的最高有效字节（Most Significant Byte, MSB）存储在最低的内存地址处,而最低有效字节（Least Significant Byte, LSB）则存储在最高的内存地址.例如,十六进制数0x12345678在大端序中存储为：

```
地址  |  数据
0x00  |  0x12
0x01  |  0x34
0x02  |  0x56
0x03  |  0x78
```

相对而言,在小端序中,数据的最低有效字节存储在最低的内存地址,而最高有效字节存储在最高的内存地址.使用同样的例子,0x12345678在小端序中存储为：

```
地址  |  数据
0x00  |  0x78
0x01  |  0x56
0x02  |  0x34
0x03  |  0x12
```

不同的计算机体系结构可能使用不同的字节序.例如,Intel架构的处理器通常采用小端序,而某些网络协议和其他架构（如IBM的PowerPC）则使用大端序.这种差异在数据传输和存储时可能导致兼容性问题,尤其是在不同平台之间交换数据时.

为了确保数据在不同字节序的系统之间能够正确解析,开发者需要在编程时考虑字节序的转换.例如,在网络编程中,通常使用网络字节序（大端序）进行数据传输,而在本地处理时可能需要将其转换为主机字节序（可能是大端或小端）.

总之,字节序是计算机体系结构中的一个重要特性,它影响着数据的存储和传输方式,理解这一点对于跨平台开发和数据交换至关重要.

</details>

### 1.1.2 电影,电视和视频

<details>
<summary>如何理解普遍认为,电影的发明是源于人们发现了视觉暂留现象￼.后来,人们又发现用“似动现象”￼来解释电影原理似乎更为合理</summary>
视觉暂留现象和似动现象是理解电影如何产生运动感的两个重要概念.

视觉暂留现象是指当一个图像在视网膜上消失后,眼睛仍然会在短时间内保留这个图像的印象.这种现象使得当多个静态图像快速连续播放时,观众的眼睛会将这些图像合成为一个流畅的运动.例如,早期的光学玩具如幻灯片和旋转盘（如现象盘）利用这一原理来创造运动的错觉[2](https://www.britannica.com/art/history-of-the-motion-picture)[4](https://open.lib.umn.edu/mediaandculture/chapter/8-2-the-history-of-movies/).

而似动现象则是指当一系列静态图像以一定的速度快速切换时,观众会感知到这些图像之间的运动.这种现象是基于人类视觉系统对快速变化的图像的处理能力,能够将这些图像整合为连续的运动.这种原理在电影放映中尤为重要,传统的电影放映速度为每秒24帧,这个速度足以让观众感受到流畅的运动,而不是单独的静态画面[3](https://www.pbs.org/wgbh/americanexperience/features/pickford-early-history-motion-pictures/).

因此,虽然视觉暂留现象为电影的运动感提供了基础,但似动现象更全面地解释了电影如何通过快速播放静态图像来创造出连续的动态效果.这两者共同构成了电影作为一种艺术形式的基础,使得观众能够沉浸在故事和情感之中.

</details>

<details>
<summary>为什么我们常说1080p,而不说1920p</summary>
1080p是指视频分辨率的一种标准,具体来说,它表示的是1080个垂直像素的清晰度,而“p”代表逐行扫描（progressive scan）,即图像是以完整的帧方式显示的.这个标准的全称是1920x1080,表示水平有1920个像素,垂直有1080个像素,因此1080p通常被称为全高清（Full HD）[2](https://en.wikipedia.org/wiki/1080p).

在日常使用中,提到1080p而不是1920p,主要是因为1080p这个术语已经成为行业标准和消费者熟知的标识.它不仅简洁易记,而且直接反映了视频的垂直分辨率,这对于观看体验来说是一个重要的指标.虽然1920x1080的分辨率包含了水平和垂直的像素信息,但在讨论视频质量时,垂直像素数（即1080）更能直接影响到画面的清晰度和细节表现[4](https://forums.tomshardware.com/threads/im-confused-is-1920x1200-2k-or-1080p.2493832).

</details>

> 一块屏的分辨率达到300ppi以上,我们就叫它视网膜屏.ppi（Pixels Per Inch,每英寸的像素数,也称dpi,Dots Per Inch）是描述最高分辨能力的单位

<details>
<summary>如何计算ppi</summary>
计算PPI（每英寸像素数）涉及两个主要步骤：首先计算显示屏的对角线像素数,然后将这个值除以显示屏的对角线尺寸（以英寸为单位）.

以下是详细的计算步骤：

1. **计算对角线像素数**：
   使用勾股定理来计算对角线的像素数.假设显示屏的宽度为`width`（水平像素数）,高度为`height`（垂直像素数）,则对角线像素数可以通过以下公式计算：

   $$
   \text{对角线像素数} = \sqrt{\text{width}^2 + \text{height}^2}
   $$

2. **计算PPI**：
   一旦得到了对角线像素数,接下来需要将其除以显示屏的对角线尺寸（以英寸为单位）.假设对角线尺寸为`diagonal_inches`,则PPI的计算公式为：

$$
\text{PPI} = \frac{\text{对角线像素数}}{\text{对角线尺寸（英寸）}}
$$

通过这个过程,你可以得到显示屏的PPI值,这个值越高,表示显示屏的像素密度越高,图像越清晰.

例如,如果一个显示屏的宽度为1920像素,高度为1080像素,对角线尺寸为10英寸,计算步骤如下：

1. 计算对角线像素数：

   $$
   \text{对角线像素数} = \sqrt{1920^2 + 1080^2} \approx 2202.91
   $$

2. 计算PPI：
   $$
   \text{PPI} = \frac{2202.91}{10} \approx 220.29
   $$

因此,该显示屏的PPI约为220.29[1](https://www.calculatorsoup.com/calculators/technology/ppi-calculator.php)[2](https://rows.com/calculators/pixels-per-inch-calculator).

</details>

<details>
<summary>16英寸2.5k显示器的ppi是多少</summary>

![](/assets/webp/1.webp)

要计算16英寸2.5K显示器的PPI（每英寸像素数）,首先需要知道2.5K的分辨率和显示器的对角线尺寸.

1. **确定分辨率**：
   2.5K的标准分辨率通常是2560x1440像素.

2. **计算对角线像素数**：
   使用勾股定理计算对角线像素数：

   $$
   \text{对角线像素数} = \sqrt{2560^2 + 1440^2}
   $$

   计算：

   $$
   \text{对角线像素数} = \sqrt{6553600 + 2073600} = \sqrt{8627200} \approx 2936.45
   $$

3. **计算PPI**：
   将对角线像素数除以对角线尺寸（16英寸）：
   $$
   \text{PPI} = \frac{2936.45}{16} \approx 183.53
   $$

因此,16英寸2.5K显示器的PPI约为183.53.这意味着该显示器的像素密度为183.53像素每英寸,显示效果相对清晰[2](https://www.quora.com/How-do-I-calculate-the-PPI-Pixels-per-inch-of-a-smartphone).

我的mac air的显示器分辨率显示为2560x1600,对角线尺寸为13.3英寸,那么它的ppi是多少呢？

大概是226.98

> 虽然没有达到传统的300 PPI标准,但由于**苹果的定义和实际观感**,它仍然可以被称为视网膜显示器.

</details>

<details>
<summary>运动估计和运动补偿</summary>

运动估计和运动补偿是视频压缩技术中两个关键的概念,它们通过利用视频帧之间的时间冗余来提高压缩效率.

运动估计的主要目的是识别视频中物体的运动.它通过分析连续帧之间的差异,确定哪些部分的图像发生了变化,以及这些变化的方向和幅度.通常,运动估计会生成运动向量,这些向量描述了图像中每个块（例如16×16像素的宏块）在时间上的移动情况.这一过程是视频编码中的重要步骤,因为它帮助编码器预测当前帧的内容,从而减少需要存储和传输的数据量[1](https://en.wikipedia.org/wiki/Motion_estimation)[4](https://www.sciencedirect.com/topics/engineering/motion-compensation).

运动补偿则是利用运动估计的结果来生成当前帧的近似图像.具体来说,编码器会使用之前的帧（参考帧）和运动向量来重建当前帧.通过这种方式,编码器只需存储运动向量和参考帧的差异,而不是完整的当前帧.这种方法显著减少了数据量,因为在许多情况下,连续帧之间的变化相对较小,只有少量信息需要被编码和传输[1](https://en.wikipedia.org/wiki/Motion_compensation)[6](https://en.wikipedia.org/wiki/Motion_estimation).

总的来说,运动估计和运动补偿通过有效地利用视频帧之间的相似性,显著提高了视频压缩的效率,使得视频文件在保持较高质量的同时,能够占用更少的存储空间和带宽.

</details>

<details>
<summary>运动估计和运动补偿在实际应用中是如何实现的？比如,它们是如何在不同的视频编码标准中应用的？</summary>
运动估计和运动补偿在实际应用中通过多种算法和技术实现,广泛应用于不同的视频编码标准中,如H.264、H.265（HEVC）和MPEG系列等.

在运动估计方面,常用的方法包括块匹配算法（Block Matching Algorithm, BMA）.该算法将当前帧划分为多个小块,并在参考帧中寻找与这些小块最相似的区域.通过计算块之间的相似度（如均方误差MSE）,算法可以确定最佳匹配块的位置,从而生成运动向量.这些运动向量描述了每个块在时间上的移动情况,通常以像素为单位表示[1](https://en.wikipedia.org/wiki/Motion_estimation)[4](https://www.sciencedirect.com/topics/computer-science/motion-compensation).

在运动补偿的实现中,编码器利用运动向量和参考帧来重建当前帧.具体步骤如下：首先,编码器根据运动向量从参考帧中提取相应的块,然后将这些块进行组合,形成当前帧的预测图像.接下来,编码器计算当前帧与预测图像之间的差异（残差）,并将该残差进行编码.通过这种方式,编码器只需存储运动向量和残差,而不是完整的当前帧,从而实现数据的压缩[1](https://en.wikipedia.org/wiki/Motion_compensation)[6](https://en.wikipedia.org/wiki/Motion_estimation).

在不同的视频编码标准中,运动估计和运动补偿的具体实现可能有所不同.例如,H.264标准采用了多种块大小的运动估计,允许在16×16、8×8和4×4等不同大小的块中进行运动估计,以提高编码效率.而H.265（HEVC）则进一步改进了这一点,支持更灵活的块划分（如CTU,Coding Tree Unit）,并引入了更复杂的运动补偿技术,以提高压缩比和视频质量[2](https://koreascience.kr/article/JAKO201925454133958.pdf)[4](https://www.sciencedirect.com/topics/computer-science/motion-compensation).

总之,运动估计和运动补偿通过精确的算法和灵活的实现方式,在视频编码标准中发挥着至关重要的作用,使得视频压缩技术能够高效地处理和传输大量视频数据.

</details>

<details>
<summary>如何理解需要编解码的视频图像一般不使用RGB色彩空间,而是使用一种称为YUV的色彩空间.</summary>
在视频编码和解码过程中,YUV色彩空间通常被优先使用而不是RGB色彩空间,主要是因为YUV更有效地利用了人眼对亮度和色彩的感知特性.YUV色彩空间将图像信息分为亮度（Y）和色度（U和V）两个部分,这种分离使得在传输和存储时可以更有效地压缩数据.

首先,YUV色彩空间中的Y分量代表亮度信息,而U和V分量则代表色彩信息.人眼对亮度变化的敏感度远高于对色彩变化的敏感度,因此在视频编码中,可以对色度信息进行下采样（即减少色度信息的分辨率）,而不会显著影响视觉质量.这种技术称为色度下采样（Chroma Subsampling）,例如4:2:0格式就意味着每四个亮度样本共享两个色度样本,从而有效减少了数据量[2](https://video.matrox.com/en/media/guides-articles/introduction-color-spaces-video)[3](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/Video_concepts).

其次,YUV色彩空间在视频传输中也具有更好的兼容性,尤其是在模拟电视信号的时代,YUV被设计为能够在仅支持黑白信号的系统中传输色彩信息.这使得YUV在广播和视频传输中成为标准选择,因为它可以在不影响亮度信息的情况下,传递色彩信息[1](https://dexonsystems.com/blog/rgb-yuv-color-spaces)[4](https://en.wikipedia.org/wiki/Y%E2%80%B2UV).

总的来说,YUV色彩空间的使用使得视频图像在编码和解码过程中更加高效,能够在保持视觉质量的同时减少数据带宽的需求.

</details>

<details>
<summary>RGB色彩空间的图像如何转换成YUV色彩空间的图像</summary>
要将RGB色彩空间的图像转换为YUV色彩空间的图像,可以使用以下转换公式.YUV色彩空间的转换主要依赖于亮度（Y）和色度（U和V）的计算.以下是转换的详细步骤和伪代码.

### 转换公式

从RGB到YUV的转换公式如下：

- Y = 0.299R + 0.587G + 0.114B
- U = 0.492(B - Y)
- V = 0.877(R - Y)

### 伪代码

1. 定义一个函数 `rgb_to_yuv`,接受RGB值作为输入.
2. 计算Y、U和V值.
3. 返回YUV值.

### 伪代码示例

```
函数 rgb_to_yuv(R, G, B):
    Y = 0.299 * R + 0.587 * G + 0.114 * B
    U = 0.492 * (B - Y)
    V = 0.877 * (R - Y)
    返回 (Y, U, V)
```

### 实际代码示例（Python）

下面是一个Python实现的示例代码,将RGB值转换为YUV值：

```python
def rgb_to_yuv(R, G, B):
    Y = 0.299 * R + 0.587 * G + 0.114 * B
    U = 0.492 * (B - Y)
    V = 0.877 * (R - Y)
    return (Y, U, V)

# 示例使用
R, G, B = 255, 0, 0  # 红色
Y, U, V = rgb_to_yuv(R, G, B)
print(f"Y: {Y}, U: {U}, V: {V}")
```

在这个示例中,输入的RGB值为红色（255, 0, 0）,通过调用`rgb_to_yuv`函数,可以得到对应的YUV值.这个转换过程在视频处理和图像编码中非常重要,因为它能够有效地将图像信息从RGB格式转换为YUV格式,以便进行后续的处理和压缩.

</details>

<details>
<summary>GOP的应用场景</summary>
GOP（Group of Pictures）是一种视频编码技术,广泛应用于视频压缩和传输中.其主要应用场景包括：

1. **视频压缩**：GOP结构在视频编码中用于减少数据量,提高压缩效率.通过使用关键帧（I帧）和预测帧（P帧、B帧）,GOP能够有效地减少冗余数据,从而在保持视频质量的同时减小文件大小.这在流媒体、视频会议和视频存储中尤为重要[1](https://www.videoproc.com/video-editor/what-is-gop-in-video.htm).

2. **流媒体传输**：在网络视频传输中,GOP结构可以帮助优化带宽使用.通过合理设置GOP的长度和帧类型,可以在不同网络条件下实现更流畅的视频播放体验.这对于在线视频平台和直播服务至关重要[2](https://www.streamingmedia.com/Articles/What-is-GOP-Structure-in-Video-Compression-131144.aspx).

3. **视频编辑和后期制作**：在视频编辑软件中,GOP结构影响着视频的剪辑和特效处理.编辑人员可以根据GOP的结构选择合适的帧进行编辑,以实现更高效的工作流程和更好的视频效果[3](https://www.premiumbeat.com/blog/understanding-gop-structure-in-video-compression/).

4. **视频监控**：在视频监控系统中,GOP结构可以帮助提高存储效率和回放质量.通过设置合适的GOP参数,可以在保证监控画面清晰度的同时,减少存储空间的占用[4](https://www.dahua.com/support/faq/faq_20200709_0001).

5. **数字广播**：在数字电视和广播中,GOP结构用于编码和传输视频信号,以确保高质量的图像和声音传输.合理的GOP设置可以提高信号的抗干扰能力,确保观众获得良好的观看体验[5](https://www.itu.int/en/ITU-T/studygroups/2017-2020/16/Pages/gop.aspx).

通过这些应用场景,可以看出GOP在现代视频技术中扮演着重要角色,影响着视频的质量、存储和传输效率.

</details>

<details>
<summary>如何理解I帧,P帧和B帧</summary>

I帧、P帧和B帧是视频编码中的三种基本帧类型,它们在视频压缩中扮演着重要角色.

I帧（Intra-coded Frame）是完全自包含的帧,类似于JPEG图像.它只使用该帧内的信息进行压缩,不依赖于其他帧的数据.这种帧通常用于视频序列的起始部分,提供完整的图像信息,因此在解码时可以独立重建.I帧的压缩效率较低,但它是其他帧（如P帧和B帧）的参考帧,直接影响后续帧的质量[1](https://ottverse.com/i-p-b-frames-idr-keyframes-differences-usecases/)[4](https://www.go-rbcs.com/articles/h264-part2).

P帧（Predicted Frame）是预测帧,它依赖于之前的I帧或P帧进行数据压缩.P帧只存储与前一帧之间的差异信息,利用时间预测来减少数据量.这种帧可以通过参考之前的帧来有效地压缩视频数据,从而提高编码效率[2](https://blog.otterbro.com/frame-types-framing-the-whole-picture/).

B帧（Bidirectional Predicted Frame）是双向预测帧,它不仅可以参考之前的帧,还可以参考之后的帧.这种帧通过同时利用前后帧的信息来进行压缩,通常能够提供最高的压缩率,但编码和解码的复杂性也相对较高.B帧的灵活性使其在视频压缩中非常有效,但也会增加处理的计算负担[3](https://fmuser.org/news/IPTV-encoder/Introduction-of-I-frame-P-frame-B-frame/).

总的来说,I帧、P帧和B帧通过不同的方式进行数据压缩和预测,形成了视频编码的基础,帮助在保证视频质量的同时减少文件大小.

</details>

### 1.1.3 音频

<details>
<summary>tone && p5 制作音轨</summary>

频谱条

import Tone from "@components/react/p5/little/tone.tsx";

<Tone hideRandom client:visible />

```tsx
import type p5 from "p5";
import * as Tone from "tone";
import React from "react";
import Basic from "../index.tsx";

export default () => {
  const sketch = (p: p5) => {
    let fft, player;
    let frame = 0;
    const setup = async () => {
      await Tone.start();
      player = new Tone.Player("/assets/sorry.mp3").toDestination();
      fft = new Tone.FFT(256).toDestination();
      player.connect(fft);
      p.createCanvas(p.windowWidth / 2, 240);
      player.autostart = true;
    };
    const draw = () => {
      p.background(200);
      // 获取频谱数据
      let spectrum = fft.getValue();
      // 绘制频谱
      p.noStroke();
      p.fill(0, 255, 0); // 绿色
      for (let i = 0; i < spectrum.length; i++) {
        let x = p.map(i, 0, spectrum.length, 0, p.width);
        let h = p.map(spectrum[i], -100, 0, 0, p.height);
        p.rect(x, p.height, p.width / spectrum.length, -h);
      }
    };
    const resize = () => p.setup();
    p.setup = setup;
    p.draw = draw;
    p.windowResized = resize;
  };
  return <Basic sketch={sketch}></Basic>;
};
```

正弦表示

<Tone hideRandom client:visible curve />

改进组件

```tsx
import type p5 from "p5";
import * as Tone from "tone";
import Basic from "../index.tsx";
import { Button } from "@shadcn/ui/button";

export default ({ curve }: { curve?: Boolean }) => {
  let fft,
    player: Tone.Player,
    drawMe = false,
    waveform;

  let play = () => {
    player && player.start();
    drawMe = true;
  };
  let pause = () => {
    player && player.stop();
    drawMe = false;
  };
  const sketch = (p: p5) => {
    const setup = async () => {
      await Tone.start();
      player = new Tone.Player("/assets/sorry.mp3").toDestination();
      fft = new Tone.FFT(2048).toDestination();
      waveform = new Tone.Waveform(2048);
      if (curve) player.connect(waveform);
      else player.connect(fft);
      p.createCanvas(p.windowWidth - 100, 240);
      p.frameRate(120); // 设置帧率
    };
    const draw = () => {
      if (drawMe && fft && waveform) {
        p.background(255, 255, 255);
        p.strokeWeight(4);
        p.stroke(255);
        // 绘制频谱
        if (!curve) {
          let spectrum = fft.getValue();
          p.noStroke();
          p.fill(0, 0, 0);
          for (let i = 0; i < spectrum.length; i++) {
            let x = p.map(i, 0, spectrum.length, 0, p.width);
            let h = p.map(spectrum[i], -100, 0, 0, p.height);
            // 绘制频谱条
            p.rect(x, p.height, p.width / spectrum.length, -h);
          }
        } else {
          let wave = waveform.getValue();
          // 绘制正弦曲线
          let colorValue = p.map(wave[0], -1, 1, 0, 255);
          let c = p.noise(p.frameCount);
          p.stroke(colorValue, 255 - colorValue, c * 255);
          p.noFill();
          p.beginShape();
          for (let i = 0; i < wave.length; i++) {
            let x = p.map(i, 0, wave.length, 0, p.width);
            let y = p.map(wave[i], -1, 1, p.height, 0); // 反转 y 轴
            p.vertex(x, y);
          }
          p.endShape();
        }
      }
    };
    const resize = () => p.setup();
    p.setup = setup;
    p.draw = draw;
    p.windowResized = resize;
  };
  return (
    <>
      <Basic sketch={sketch}></Basic>
      <div className="flex gap-10">
        <Button className="m-auto" onClick={play}>
          播放
        </Button>
        <Button className="m-auto" onClick={pause}>
          结束
        </Button>
      </div>
    </>
  );
};
```

</details>

<details>
<summary>如何理解音频信号的压缩原理</summary>
音频信号的压缩原理主要是通过动态范围压缩（Dynamic Range Compression）来实现的。这一过程旨在减少音频信号中最响和最静的部分之间的音量差异，从而使得整体音频更加平衡和易于听取。压缩器会根据设定的阈值（Threshold）来决定何时开始压缩信号。只有当音频信号的强度超过这个阈值时，压缩器才会对其进行处理。

压缩器的工作原理可以通过几个关键参数来理解：

1. **阈值（Threshold）**：这是压缩器开始工作的音量水平。当音频信号超过这个水平时，压缩器会开始降低信号的音量。

2. **比率（Ratio）**：比率决定了信号超过阈值后，压缩器将其降低的程度。例如，4:1的比率意味着如果输入信号超过阈值4 dB，输出信号将仅为1 dB超过阈值。

3. **攻击时间（Attack Time）**：这是压缩器响应信号超过阈值所需的时间。较快的攻击时间可以使压缩器迅速反应，而较慢的攻击时间则允许更多的瞬态信号通过。

4. **释放时间（Release Time）**：这是压缩器在信号降到阈值以下后，恢复到原始状态所需的时间。适当的释放时间可以避免音频信号出现“泵动”效应。

5. **输出增益（Output Gain）**：由于压缩会导致整体音量降低，输出增益用于补偿这一点，使得压缩后的信号听起来更响亮。

通过合理设置这些参数，音频压缩可以帮助提升录音的专业性，使得音轨听起来更具冲击力和一致性，同时避免过度压缩导致的音质损失[2](https://www.uaudio.com/blog/audio-compression-basics/)[3](https://en.wikipedia.org/wiki/Dynamic_range_compression)。

</details>

### 1.1.4 音视频封装、传输和未

<details>
<summary>如何理解RTMP流一般用于CDN推拉流,也是音频和视频交错发送</summary>

RTMP（Real-Time Messaging Protocol）是一种用于音频、视频和数据在互联网上实时传输的协议.它最初由Adobe开发,广泛应用于流媒体传输,尤其是在内容分发网络（CDN）中.RTMP流的特点之一是支持音频和视频的交错发送,这对于实时播放至关重要.

在CDN推拉流的场景中,RTMP流可以通过两种方式进行传输：推流和拉流.推流是指将音视频数据从源（如直播编码器）推送到CDN服务器,而拉流则是CDN服务器从源服务器拉取音视频数据.这种灵活性使得RTMP成为实时流媒体应用的理想选择[1](https://cdnsun.com/solutions/rtmp-streaming)[4](https://cdnsun.com/knowledgebase/cdn-live/differences-between-pull-and-push-publishing).

音频和视频的交错发送是RTMP流的一个重要特性.在RTMP流中,音频和视频数据不会单独发送,而是交替发送.这意味着在同一个数据流中,音频包和视频包交替出现,这样可以确保播放器在接收数据时能够同时获取音频和视频信息,从而实现同步播放.这种交错发送的机制有助于减少延迟,提高流媒体播放的流畅性和实时性[2](https://www.wowza.com/community/t/how-to-push-wowza-rtmp-stream-to-cdn/34982)[5](https://docs.agora.io/en/3.x/interactive-live-streaming/advanced-features/media-push).

总之,RTMP流在CDN推拉流中发挥着重要作用,通过交错发送音频和视频数据,确保了实时流媒体的高效传输和同步播放.

</details>

<details>
<summary>如何理解SIP及WebRTC通信的实时性更好一些，使用RTP流传输，音频和视频使用不同的流（不同的端口号）发送，有时为了节省端口号也会合并到一个流上发送。</summary>
SIP（会话发起协议）和WebRTC（网页实时通信）都是用于实时通信的技术，但它们在实现和应用上有所不同。理解这两者的实时性及其在RTP（实时传输协议）流传输中的应用，可以帮助我们更好地选择合适的技术。

首先，SIP是一种信令协议，主要用于建立、修改和终止多媒体会话。它可以与RTP结合使用，RTP负责传输音频和视频流。SIP本身并不传输媒体数据，而是负责会话的控制和管理。通过SIP，用户可以在不同的设备之间建立连接，并使用RTP进行音视频数据的传输。通常情况下，音频和视频会使用不同的RTP流（即不同的端口号）进行发送，这样可以更好地管理带宽和流量，确保音视频的质量和实时性。

另一方面，WebRTC是一种开源技术，允许浏览器和移动应用程序通过简单的API实现实时通信。WebRTC本身集成了音视频传输的功能，使用RTP进行流传输。WebRTC的优势在于其无需插件，能够直接在浏览器中实现点对点的音视频通信。虽然WebRTC也可以将音频和视频流合并到一个RTP流中以节省端口号，但这可能会影响到流的管理和质量。因此，WebRTC通常会选择使用多个流来确保更高的实时性和更好的音视频质量。

总结来说，SIP和WebRTC在实时性方面各有优势。SIP适合需要与传统电话系统集成的场景，而WebRTC则更适合需要快速部署和用户友好的应用。选择使用不同的RTP流或合并流的策略，取决于具体的应用需求和网络环境。

</details>

<details>
<summary>如何理解立体声，不管有几个声道，本质上还是2D的声场</summary>
立体声（Stereo）是指通过两个或多个独立的音频通道来再现声音的技术，通常使用左声道和右声道来创建音频的空间感。尽管立体声可以通过多个声道来传递声音，但其本质上仍然是一个二维（2D）的声场。这是因为立体声的空间定位主要依赖于左右声道的音量差异和时间差异来模拟声音的方向感。

在立体声中，声音的定位是通过人耳接收到的声音强度和到达时间的差异来实现的。例如，当一个声音从左侧发出时，左耳会比右耳更早接收到声音，同时左耳接收到的声音强度也会更大。这种差异使得我们能够感知声音的来源方向，从而在脑中形成一个二维的声场，即左右的空间感。

然而，立体声的局限性在于它无法提供真正的三维（3D）音场体验。虽然可以通过不同的技术（如环绕声或3D音频技术）来增强空间感，但传统的立体声仍然无法完全模拟声音在三维空间中的位置和移动。因此，尽管立体声可以创造出丰富的音频体验，但其本质上仍然是基于二维的声场概念，主要通过左右声道的组合来实现声音的定位和空间感[2](https://unlockyoursound.com/stereo)[3](https://unison.audio/stereo-vs-mono/)。

</details>

### 1.2.1 图像的位深

<details>
<summary>如何理解图像的位深</summary>
位深（Bit Depth）是数字图像处理中一个重要的概念，它指的是每个像素能够表示的颜色信息的位数。位深越高，图像能够表示的颜色或灰度级别就越多，从而使图像的细节和色彩表现更加丰富。

在数字图像中，位深通常以每个像素的位数来表示。例如，1位的图像只能表示两种颜色（通常是黑和白），而8位的图像可以表示256种不同的颜色或灰度级别。对于彩色图像，常见的位深为24位，这意味着每个像素由三个颜色通道（红、绿、蓝）组成，每个通道8位，因此可以表示超过1670万种颜色（2^24）[1](http://preservationtutorial.library.cornell.edu/tutorial/intro/intro-04.html)[2](https://www.cambridgeincolour.com/tutorials/bit-depth.htm)。

位深的选择对图像的质量和文件大小有直接影响。较高的位深可以捕捉更多的色彩细节，特别是在后期处理时，能够更好地保留图像的动态范围和色彩渐变。然而，较高的位深也会导致文件大小增加，因此在实际应用中需要根据需求进行权衡[3](https://www.nps.gov/museum/publications/conserveogram/22-01.pdf)[5](https://helpx.adobe.com/photoshop/using/bit-depth.html)。

总之，理解位深的关键在于认识到它如何影响图像的色彩表现和文件大小，以及在不同应用场景下选择合适的位深以达到最佳效果。

</details>

### 1.2.2 FourCC￼代码

<details>
<summary>啥是FourCC</summary>

FourCC（Four Character Code）是一种用于标识视频和音频编码格式的四个字符的代码。它通常用于多媒体文件中，以便于软件和硬件识别和处理不同的数据格式。FourCC代码是一个32位的无符号整数，由四个ASCII字符连接而成，例如，YUY2视频格式的FourCC代码是'YUY2'。

在编程中，FourCC代码可以通过一些宏来简化声明。例如，在C/C++中，可以使用`MAKEFOURCC`宏来创建FourCC值，如下所示：

```c
DWORD fccYUY2 = MAKEFOURCC('Y', 'U', 'Y', '2');
```

在Windows操作系统中，由于其小端架构，FourCC代码的字符顺序在内存中是反向存储的。例如，'YUY2'在内存中的表示为'2YUY'，其十六进制值为0x32595559。这种表示方式在处理视频格式时非常重要，因为它确保了正确的格式识别和解码。

FourCC代码广泛应用于视频编解码器、图形控制器芯片以及多媒体文件的搜索和管理中。例如，许多视频格式都有相应的FourCC代码，以便在不同的应用程序和设备之间进行兼容性处理[1](https://softron.zendesk.com/hc/en-us/articles/207695697-List-of-FourCC-codes-for-video-codecs)[3](https://learn.microsoft.com/en-us/windows/win32/directshow/fourcc-codes)[4](https://learn.microsoft.com/en-us/windows/win32/medfound/video-fourccs)。

</details>

<details>
<summary>常见的FourCC代码</summary>

常见的FourCC代码包括多种视频和音频编码格式。以下是一些常见的FourCC代码及其对应的编码格式：

1. **YUY2** - YUY2视频格式
2. **MJPG** - Motion JPEG
3. **H264** - H.264视频编码
4. **AVC1** - H.264/AVC视频编码
5. **DV** - DV视频格式
6. **MP4V** - MPEG-4视频编码
7. **XVID** - Xvid视频编码
8. **DIVX** - DivX视频编码
9. **WMV3** - Windows Media Video 9
10. **VP80** - VP8视频编码

这些FourCC代码在多媒体文件中用于标识不同的编码格式，以便于播放器和编辑软件进行正确的解码和播放[1](https://softron.zendesk.com/hc/en-us/articles/207695697-List-of-FourCC-codes-for-video-codecs)[3](https://learn.microsoft.com/en-us/windows/win32/medfound/video-fourccs)。

</details>

### 1.2.6 HSL与HSV色彩表示

<details>
<summary>概述</summary>

HSL（色相、饱和度、亮度）和HSV（色相、饱和度、值）是两种常见的色彩表示模型，它们在计算机图形学和图像处理领域被广泛使用。这两种模型都试图通过更直观的方式来表示颜色，相较于传统的RGB模型，它们更符合人类对颜色的感知。

HSL模型中的三个参数分别是色相（Hue）、饱和度（Saturation）和亮度（Lightness）。色相表示颜色的类型，例如红色、绿色或蓝色，通常用角度（0°到360°）表示。饱和度表示颜色的纯度，值越高颜色越鲜艳，值越低则颜色趋向于灰色。亮度则表示颜色的明暗程度，从黑色（0）到白色（1）之间变化[2](https://en.wikipedia.org/wiki/HSL_and_HSV)[3](https://www.w3schools.com/colors/colors_hsl.asp)。

HSV模型的参数与HSL相似，但有一些关键的区别。色相的定义与HSL相同，饱和度的定义也相似，但在HSV中，饱和度的变化更直接影响颜色的亮度。值（Value）在HSV中表示颜色的亮度，值越高颜色越亮，值越低则颜色越暗[4](https://www.nixsensor.com/blog/what-is-hsl-color/)[5](https://cloudfour.com/thinks/hsl-a-color-format-for-humans/)。这使得HSV在某些应用中更适合用于选择和调整颜色，尤其是在图像编辑软件中。

总的来说，HSL和HSV都是为了提供一种更符合人类视觉感知的方式来表示颜色，但它们在饱和度和亮度的定义上有所不同，适用于不同的应用场景。

</details>

<details>
<summary>demo</summary>

<div class="bg-[hsl(0,0%,50%)]">background-color: hsl(0, 0%, 50%);</div>
<div class="bg-[hsl(0,0%,100%)]">background-color: hsl(0, 0%, 100%);</div>
<div class="bg-[hsl(0,100%,50%)]">background-color: hsl(0, 100%, 50%);</div>
<div class="bg-[hsl(0,100%,100%)]">background-color: hsl(0, 100%, 100%);</div>
<div class="bg-[hsl(120,100%,50%)]">background-color: hsl(120, 100%, 50%);</div>
<div class="bg-[hsl(120,100%,100%)]">
  background-color: hsl(120, 100%, 100%);
</div>
<div class="bg-[hsl(210,100%,50%)]">background-color: hsl(210, 100%, 50%);</div>

hsl自定义

import HslSelect from "@components/react/little/hlsSelect.tsx";

<HslSelect client:visible />

hsl结合conic-gradient

<div class="w-[100px] h-[100px] bg-hc"></div>

<style is:inline>.bg-hc\{background: conic-gradient(hsl(360 100% 50%),hsl(315 100% 50%),hsl(270 100% 50%),hsl(225 100% 50%),hsl(180 100% 50%),hsl(135 100% 50%),hsl(90 100% 50%),hsl(45 100% 50%),hsl(0 100% 50%));clip-path: circle(closest-side);}</style>

</details>
